{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "def scrollDown(driver, reviewTotal, wait):\n",
    "    x = len(driver.find_elements_by_xpath(\"//div[@class='gws-localreviews__general-reviews-block']//div[@class='WMbnJf gws-localreviews__google-review']\"))\n",
    "    lastx = x - 1\n",
    "    i = 1\n",
    "    repCount = 0\n",
    "    print(\"scrolling...\")\n",
    "    # while we expect more reviews and we haven't repeated too much\n",
    "    while ((x < reviewTotal) and (repCount < 3)):\n",
    "        # pause to not look like a bot\n",
    "        pauseScroll(wait)\n",
    "        # select list of reviews\n",
    "        elements = driver.find_elements_by_xpath(\"//div[@class='gws-localreviews__general-reviews-block']//div[@class='WMbnJf gws-localreviews__google-review']\")\n",
    "        # find the last visible review and go to it\n",
    "        current_last = elements[-1]\n",
    "        current_last.location_once_scrolled_into_view\n",
    "        # wait until page loads\n",
    "        WebDriverWait(driver, 30).until(ec.invisibility_of_element_located((By.XPATH, \"//div[@class='jfk-activityIndicator-icon']\")))\n",
    "        # take the previous review count and replace it with the new one\n",
    "        lastx = x\n",
    "        x = len(driver.find_elements_by_xpath(\"//div[@class='gws-localreviews__general-reviews-block']//div[@class='WMbnJf gws-localreviews__google-review']\"))\n",
    "        # if more reviews are not added to the visible reviews, mark repetition\n",
    "        if (lastx == x):\n",
    "            repCount += 1\n",
    "        else:\n",
    "            repCount = 0\n",
    "            \n",
    "    print(\"finished scrolling, found\", x, \"reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReviewTotal(driver):\n",
    "    raw = driver.find_element_by_xpath(\"//span[@class='hqzQac']\").text\n",
    "    count = int(raw.split(\" \")[0])\n",
    "    print(\"expecting\", count, \"reviews\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "import time\n",
    "def pauseScroll(wait):\n",
    "    waitTime = 0\n",
    "    # if argument is a list\n",
    "    if (isinstance(wait, list)):\n",
    "        # check if its length is valid\n",
    "        if (len(wait) > 2) or (len(wait) < 1):\n",
    "            raise ValueError\n",
    "        # if length is valid get random number between vals\n",
    "        low = wait[0]\n",
    "        high = wait[1]\n",
    "        # check for improper types, fix if possible\n",
    "        if not isinstance(low, numbers.Number):\n",
    "            if low.isnumeric():\n",
    "                low = float(low)\n",
    "            else:\n",
    "                raise ValueError\n",
    "        if not isinstance(high, numbers.Number):\n",
    "            if high.isnumeric():\n",
    "                high = float(high)\n",
    "            else:\n",
    "                raise ValueError\n",
    "        waitTime = randTime(low, high)\n",
    "    # if argument is a number\n",
    "    elif (isinstance(wait, numbers.Number)):\n",
    "        # get the number\n",
    "        waitTime = wait\n",
    "    # if argument is a string\n",
    "    elif (isinstance(wait, str)):\n",
    "        # if argument can be parsed as a number\n",
    "        if (wait.isnumeric()):\n",
    "            # get time as a number\n",
    "            waitTime = float(wait)\n",
    "        else:\n",
    "            raise ValueError\n",
    "    time.sleep(waitTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "def randTime(low, high):\n",
    "    val = random()\n",
    "    time = low + (val  * (high - low))\n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "# note that this assumes scrolling has already occurred\n",
    "def scrapeFromList(driver, key, name=True, stars=True, text=True, timeSince=True):\n",
    "    elements = driver.find_elements_by_xpath(\"//div[@class='gws-localreviews__general-reviews-block']//div[@class='WMbnJf gws-localreviews__google-review']\")\n",
    "    columnsBin = [name, stars, text, timeSince]\n",
    "    columnsNames = [\"name\", \"stars\", \"text\", \"timeSince\"]\n",
    "    columnsPresent = [x for i, x in enumerate(columnsNames) if columnsBin[i]] # gets included columns\n",
    "    print(\"preparing to scrape location with key\", key)\n",
    "    \n",
    "    # text body\n",
    "    textList = []\n",
    "    if (text):\n",
    "        textPath = \".//div[@style='vertical-align:top']//div[@class='Jtu6Td']//span[@jscontroller='P7L8k']\"\n",
    "        textPathFullExtension = \"//span[@class='review-full-text']\"\n",
    "        textList = [0]*len(elements)\n",
    "        i = 0\n",
    "        for e in elements:\n",
    "            try:\n",
    "                textPartial = e.find_element_by_xpath(textPath+textPathFullExtension)\n",
    "                e.find_element_by_xpath(textPath+\"//span[@role='button']\").click()\n",
    "                textList[i] = e.find_element_by_xpath(textPath).text\n",
    "            except NoSuchElementException:\n",
    "                textList[i] = e.find_element_by_xpath(textPath).text\n",
    "            i += 1\n",
    "        \n",
    "    # name\n",
    "    nameList = []\n",
    "    if (name):\n",
    "        namePath = \".//div[@class='TSUbDb']\"\n",
    "        nameList = [e.find_element_by_xpath(namePath).text for e in elements]\n",
    "    \n",
    "    # stars\n",
    "    starsList = []\n",
    "    if (stars):\n",
    "        starsPath = \".//div[@style='vertical-align:top']//div[@class='PuaHbe']//g-review-stars[@style='padding-right:7px']//span[@class='Fam1ne EBe2gf']\"\n",
    "        i = 0\n",
    "        starsList = [0]*len(elements)\n",
    "        for e in elements:\n",
    "            raw = e.find_element_by_xpath(starsPath).get_attribute(\"aria-label\")\n",
    "            starCount = raw.split(\" \")[1] # the number of stars\n",
    "            starsList[i] = float(starCount)\n",
    "            i += 1\n",
    "    \n",
    "    # timeSince\n",
    "    timeSinceList = []\n",
    "    if (timeSince):\n",
    "        timeSincePath = \".//div[@style='vertical-align:top']//div[@class='PuaHbe']\"\n",
    "        timeSinceList = [e.find_element_by_xpath(timeSincePath).text for e in elements]\n",
    "    \n",
    "    # put df together\n",
    "    columnsBin = [name, stars, text, timeSince]\n",
    "    columnsNames = [\"name\", \"stars\", \"text\", \"timeSince\"]\n",
    "    columnsPresent = [x for i, x in enumerate(columnsNames) if columnsBin[i]] # gets included columns\n",
    "    columns = [nameList, starsList, textList, timeSinceList]\n",
    "    columns = [c for c in columns if len(c) > 0]\n",
    "    data = {c:columns[i] for i,c in enumerate(columnsPresent)}\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"scraped location with key\", key, \"with\", len(df), \"reviews\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "def scrapeFromURLs(urls, combine=True, wait=[2,3]):\n",
    "    # check that urls are dicts\n",
    "    if not (isinstance(urls, dict)):\n",
    "        raise ValueError\n",
    "        \n",
    "    # start selenium stuff\n",
    "    driver = webdriver.Chrome('./chromedriver')\n",
    "    \n",
    "    dfs = [0]*len(urls)\n",
    "    \n",
    "    # for each id\n",
    "    r_count = 0\n",
    "    i = 0\n",
    "    for id in urls:\n",
    "        url = urls.get(id)\n",
    "        driver.get(url) # go to url\n",
    "        driver.find_element_by_class_name(\"hqzQac\").click()\n",
    "        #WebDriverWait(driver, 10).until(ec.presence_of_element_located((By.XPATH, \"//div[@class='AU64fe']\")))\n",
    "        WebDriverWait(driver, 45).until(ec.presence_of_element_located((By.XPATH, \"//div[@class='gws-localreviews__general-reviews-block']//div[@class='WMbnJf gws-localreviews__google-review']\")))\n",
    "        \n",
    "        # scroll to bottom\n",
    "        scrollDown(driver, getReviewTotal(driver), wait)\n",
    "        \n",
    "        # scrape from loaded list\n",
    "        df = scrapeFromList(driver, id)\n",
    "        df[\"key\"] = str(id)\n",
    "        \n",
    "        dfs[i] = df\n",
    "        r_count += len(df)\n",
    "        i += 1\n",
    "        \n",
    "    # combine dfs if necessary\n",
    "    if (combine):\n",
    "        df = dfs[0]\n",
    "        # append all other dfs to the first one, then save to csv\n",
    "        for i in dfs[1:]:\n",
    "            df = df.append(i, ignore_index=True)\n",
    "        r_count_final = len(df)\n",
    "        df.to_csv(\"combined_scrape_\"+str(datetime.datetime.now()).replace(' ','_').replace(':','_').replace('.','_')+\".csv\",index=False)\n",
    "    else:\n",
    "        for i in dfs:\n",
    "            i.to_csv(\"scraped_\"+str(datetime.datetime.now()).replace(' ','_').replace(':','_').replace('.','_')+\".csv\",index=False)\n",
    "    \n",
    "    # for testing if rows are lost\n",
    "    print(\"sum of all rows:\",r_count)\n",
    "    print(\"total rows:\",r_count_final)\n",
    "    \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# make scrapeFromURLs take in a filename convention and have csvs save as it\n",
    "# add check for correct address\n",
    "# check that reviews button is there, i.e. check whether there is actually a location found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def extractURLs_csv(csv, searchTerms, key, firstIndex, lastIndex):\n",
    "    baseString = \"https://www.google.com/search?q=\"\n",
    "    \n",
    "    # read in csv (maybe add value checks later)\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # makes sure key is a list so it can be added to searchTerms\n",
    "    if (not isinstance(key, list)):\n",
    "        key = [key]\n",
    "    \n",
    "    search = df.loc[list(range(firstIndex,lastIndex+1)), searchTerms + key] # gets df with only search terms in desired rows\n",
    "    \n",
    "    search[\"url\"] = baseString + search[searchTerms[0]] # column of base plus first search term\n",
    "    # creates a column of url by adding on all other terms\n",
    "    for s in searchTerms[1:]:\n",
    "        search[\"url\"] = search[\"url\"] + \"+\" + search[s].astype(s)\n",
    "        \n",
    "    # converts key back into string\n",
    "    key = key[0]\n",
    "    \n",
    "    # returns dictionary with key of key and value of url\n",
    "    return dict(zip(list(search[key]),list(search[\"url\"])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlrd\n",
    "def extractURLs_excel(xlsx, searchTerms, key, firstIndex, lastIndex):\n",
    "    baseString = \"https://www.google.com/search?q=\"\n",
    "    \n",
    "    # read in csv (maybe add value checks later)\n",
    "    df = pd.read_excel(xlsx)\n",
    "    \n",
    "    # makes sure key is a list so it can be added to searchTerms\n",
    "    if (not isinstance(key, list)):\n",
    "        key = [key]\n",
    "    \n",
    "    search = df.loc[list(range(firstIndex,lastIndex+1)), searchTerms + key] # gets df with only search terms in desired rows\n",
    "    \n",
    "    search[\"url\"] = baseString + search[searchTerms[0]] # column of base plus first search term\n",
    "    # creates a column of url by adding on all other terms\n",
    "    for s in searchTerms[1:]:\n",
    "        search[\"url\"] = search[\"url\"] + \"+\" + search[s].astype(str)\n",
    "        \n",
    "    # converts key back into string\n",
    "    key = key[0]\n",
    "    \n",
    "    # returns dictionary with key of key and value of url\n",
    "    return dict(zip(list(search[key]),list(search[\"url\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {'Name':['Tom', 'nick', 'krish', 'jack'], 'Age':[20, 21, 19, 18]} \n",
    "df = pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"NameAge\"] = df[\"Name\"] + df[\"Age\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tom': 20, 'nick': 21, 'krish': 19, 'jack': 18}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdic = dict(zip(list(df[\"Name\"]), list(df[\"Age\"])))\n",
    "testdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchTerms = [\"Company Name\", \"Address 1\", \"City\", \"State\"]\n",
    "key = \"Company ID\"\n",
    "sourceFile = \"workfile.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdict2 = extractURLs_excel(\"workfile.xlsx\", searchTerms, key, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleScraper(sourceFile, searchTerms, key, firstIndex, lastIndex, wait=[2,3], combine=True):\n",
    "    \n",
    "    # get url dict\n",
    "    ext = sourceFile.split(\".\")[-1] # file extension\n",
    "    urls = {}\n",
    "    if (ext == \"xlsx\"):\n",
    "        urls = extractURLs_excel(sourceFile, searchTerms, key, firstIndex, lastIndex)\n",
    "    elif (ext == \"csv\"):\n",
    "        urls = extractURLs_csv(sourceFile, searchTerms, key, firstIndex, lastIndex)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    # scrape and save\n",
    "    scrapeFromURLs(urls, combine, wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expecting 379 reviews\n",
      "scrolling...\n",
      "finished scrolling, found 378 reviews\n",
      "preparing to scrape location with key 1000804749.0\n",
      "scraped location with key 1000804749.0 with 378 reviews\n",
      "expecting 214 reviews\n",
      "scrolling...\n",
      "finished scrolling, found 213 reviews\n",
      "preparing to scrape location with key 1000804753.0\n",
      "scraped location with key 1000804753.0 with 213 reviews\n",
      "expecting 249 reviews\n",
      "scrolling...\n",
      "finished scrolling, found 248 reviews\n",
      "preparing to scrape location with key 1000804756.0\n",
      "scraped location with key 1000804756.0 with 248 reviews\n",
      "expecting 153 reviews\n",
      "scrolling...\n",
      "finished scrolling, found 153 reviews\n",
      "preparing to scrape location with key 1000804783.0\n",
      "scraped location with key 1000804783.0 with 153 reviews\n",
      "expecting 94 reviews\n",
      "scrolling...\n",
      "finished scrolling, found 94 reviews\n",
      "preparing to scrape location with key 1000804785.0\n",
      "scraped location with key 1000804785.0 with 94 reviews\n",
      "expecting 141 reviews\n",
      "scrolling...\n",
      "finished scrolling, found 141 reviews\n",
      "preparing to scrape location with key 1000804789.0\n",
      "scraped location with key 1000804789.0 with 141 reviews\n",
      "sum of all rows: 1227\n",
      "total rows: 1227\n"
     ]
    }
   ],
   "source": [
    "simpleScraper(sourceFile, searchTerms, key, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('./chromedriver')\n",
    "driver.get(testdict2.get(1000804749.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.google.com/search?q=Pizza Hut+142 Highway 62+ASH FLAT+AR'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdict2.get(1000804749.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
